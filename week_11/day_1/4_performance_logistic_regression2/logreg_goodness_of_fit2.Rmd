---
title: "Model performance - logistic regression"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
```
# Learning Objectives

* Be able to plot and interpret a receiver operating characteristic (ROC) curve and calculate the area under the curve (AUC) and the Gini coefficient
* See cross validation applied to logistic regression
* Understand that expected value calculations can be used to set an optimal threshold

<hr>

# Recap

In the last lesson we learnt about some new terms:

* The **true positive rate** $$TPR = \frac{TP}{TP + FN}$$This can be thought of as *the proportion of actual positive cases that are correctly identified as positive by the classifier*. Also called the **sensitivity** or **recall**. 
* The **true negative rate** $$TNR = \frac{TN}{TN + FP}$$This is *the proportion of actual negative cases that are correctly identified as negative by the classifier*. Also called the **specificity**. 
* The **false positive rate** $$FPR = \frac{FP}{FP + TN}$$This corresponds to the *rate of false alarms*, i.e. *the proportion of actual negative cases that are incorrectly identified as positive by the classifier*. Also called the **fall-out**, or **type-I error rate** or **alpha** (both in hypothesis testing).
* The **false negative rate** $$FNR = \frac{FN}{FN + TP}$$ also called the **type-II error rate** or **beta** in hypothesis testing. 

**Accuracy** is "how often is the classifier correct?". This is the number of true positives $TP$ plus the number of true negatives $TN$, divided by the total number of outcomes in the sample data $N = TP + FP + TN + FN$, where $FP$ is the number of false positives and $FN$ the number of false negatives.

$$\textrm{accuracy} = \frac{TP + TN}{N}$$
**Confusion matrix** is a table containing the values for $TP, FP, TN$ and $TP$.  

Now let's learn about some ways we can visually represent some of these in order for us to compare models and assess the optimal threshold value. 


***

# Model Comparison 


If we are in a new project let's run our 3 predictor model again, using `tu_score`, `employed` and `age`. 

```{r, message=FALSE, warning = FALSE}
library(tidyverse)
library(modelr)
library(janitor)

mortgage_data <- read_csv(file = "data/mortgage_applications.csv")
mortgage_data <- clean_names(mortgage_data)

mortgage_3pred_model <- glm(accepted ~ tu_score + employed + age, data = mortgage_data, family = binomial(link = 'logit'))

mortgage_data_with_3pred <- mortgage_data %>%
  add_predictions(mortgage_3pred_model, type = "response")
head(mortgage_data_with_3pred)

summary(mortgage_3pred_model)
```

We have a few well known and used methods to compare model performance when it comes to logistic regression. They are the **ROC curve**, **AUC value**, **Gini coefficient** and **cross validation**. 

Let's look at these in turn. 


## ROC (Receiver Operator Characteristic) curves


ROC curves were developed in the Second World War by electrical engineers working on RADAR systems as a way to gauge the performance of RADAR operators to distinguish between true signals (i.e. aircraft) and noise (e.g. bird flocks) as a function of the receiver kit's 'gain' setting. 

After the war ROC curves found use in a wide variety of fields including medical testing, psychology and signal processing, and are now widely used in machine learning.

The **receiver operating characteristic (ROC) curve** is a single graphical summary of the effectiveness of a binary classification model. 

To create one, we plot $TPR$ (or sensitivity) against $1 - TNR$ (or $1 - \textrm{specificity}$) as a function of threshold probability.


The `roc()` and `ggroc()` functions from the `pROC` package let us draw ROC curves directly from data including actual and predicted classes. Function `roc()` outputs an object that we pass into `ggroc()` like so:

```{r}
library(pROC)
roc_obj_3pred <- mortgage_data_with_3pred %>%
  roc(response = accepted, predictor = pred)

roc_curve <- ggroc(data = roc_obj_3pred, legacy.axes = TRUE) +
  coord_fixed() + 
  ylab("sensitivity (TPR)") + 
  xlab("1-specificity (TNR)")

roc_curve
```

As a guide to how to interpret the plot, the closer the curve is to the top left corner, the more effective the classifier. Figure 1 shows schematic ROC curves for three classifiers of increasing effectiveness. Note that the diagonal on an ROC curve always corresponds to a 'random guessing' classifier, i.e. one for which $TPR = FPR$ (or, equivalently $\textrm{sensitivity} = 1-\textrm{specificity}$).

<br>

```{r, echo=FALSE, out.width = '60%'}
knitr::include_graphics("images/ROCcurve.png")
```

<br>
<blockquote class='task'>
**Task - 2 mins** Look at the figure above and try to answer the following questions: 

* Why are classifiers with ROC curves closer to the top-left corner said to be more effective?
* Which end of the curves corresponds to low threshold probability, and which to high?

[**Hint** - remember $1-\textrm{specificity} = FPR$]
  
<details>
<summary>**Solution**</summary>

* The 'perfect classifier' would have a $TPR$ of $1.0$ and an $FPR$ of $0.0$. This corresponds to the top-left point on the chart! It's possible for a classifier to have a curve 'below' the diagonal, in which case, it performs **more poorly** than random guessing. Quite an achievement!
* The upper right corner of the curves corresponds to **low threshold probability** (i.e. 'classify everything as positive). The lower left corner therefore corresponds to **high threshold probability**.

</details>
</blockquote>
<br>

`roc` objects also contain other useful data! We can get a list of the thresholds applied, and the sensitivities and specificities of the corresponding classifiers at each threshold level. This is useful when you want to do a more detailed analysis of which threshold to apply.

```{r}
classifier_data <- tibble(
  threshold = roc_obj_3pred$thresholds,
  sensitivity = roc_obj_3pred$sensitivities,
  specificity = roc_obj_3pred$specificities
)

head(classifier_data)
```

<br>
<blockquote class='task'>
**Task - 10 mins**

OK, let's fit another classifier and add its curve to the ROC plot from earlier!

* Fit a **single predictor** logistic regression model to the `mortgage_data`. We recommend `tu_score` as the predictor. Save the model as `mortgage_1pred_model`
* Add the predicted probabilities from this model to `mortgage_data`, and save the resulting data as `mortgage_data_with_1pred` 
* Use this data to generate an an object from `roc()`, save the object as `roc_obj_1pred` 
* Pass your old `roc_obj_3pred` and new `roc_obj_1pred` into `ggroc()` [**Hint** check the `ggroc()` docs to see how to pass in multiple `roc` objects].
* Given these ROC curves, which classifier is better?
* If you have time, try another single predictor, i.e. `age` or `employed`

<details>
<summary>**Solution**</summary>
```{r}
mortgage_1pred_model <- glm(accepted ~ tu_score, data = mortgage_data, family = binomial(link = 'logit'))

mortgage_data_with_1pred <- mortgage_data %>%
  add_predictions(mortgage_1pred_model, type = "response")

roc_obj_1pred <- mortgage_data_with_1pred %>%
  roc(response = accepted, predictor = pred)

roc_curve <- ggroc(data = list(pred3 = roc_obj_3pred, pred1 = roc_obj_1pred), legacy.axes = TRUE) +
  coord_fixed()

roc_curve
```
It looks like the 3-predictor classifier is better: it gets close to the top-left of the ROC curve, but this is quite a 'loose'definition of better.
</details>
</blockquote>
<br>

## AUC

The ROC curves give us a convenient way to compare the performance of various classifiers, we just plot them all on the same ROC plot. But it would be nice to have a single numerical measure of the effectiveness of a classifier.

In principle, if we calculate **the area under the curve** ($AUC$), we can use this as a measure of performance. Better classifiers have curves closer to the top left point, and so have **larger $AUC$ values**. The `pROC` package offers the `auc()` function to calculate the $AUC$ value for a passed in `roc()` object:

```{r}
auc(roc_obj_3pred, )
auc(roc_obj_1pred)
```

So we can see that out 3-predictor classifier `accepted ~ tu_score + age + employed` is marginally better than the single predictor classifier `accepted ~ tu_score` based upon their $AUC$ values. 

<br>
<blockquote class='task'>
**Task - 2 mins**

Remember that the random choice classifier would appear as an upwards sloping diagonal line on an ROC curve. What would the $AUC$ value be for such a classifier?

<details>
<summary>**Solution**</summary>
The area under an upwards sloping diagonal would fill **half the square**, so we would expect $AUC = 0.5$
</details>
</blockquote>
<br>

## Gini coefficient

The Gini coefficient normalises $AUC$ so that a random classifier has value $0$, and a perfect classifier has value $1$. You might see either the $AUC$ or the Gini coefficient presented as the performance measure for a classifier.

The relationship between $AUC$ and the Gini coefficient is

$\textrm{Gini} = 2 \times AUC - 1$

with a possible range of values

$-1 \le \textrm{Gini} \le 1$

Although, as for $AUC$, the practical range is

$0 \le \textrm{Gini} \le 1$

as we rarely do worse than a random choice classifier!

We can calcuate these now: 

```{r}
gini3 = 2 * 0.881 - 1
gini1 = 2 * 0.866 - 1

gini3
gini1
```

<hr>

## Cross validation

Our final method is **Cross-validation**. Cross-validation is a method that can estimate the performance of a model. It is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. This is an extremely useful technique: it gives us insight into the performance of our classifier on data not used for training, i.e. unseen data.  

Let's see how to apply cross validation to the process of fitting a logistic regression model. 

As we did last week, we will use the `caret` package for this.

```{r, message=FALSE}
library(caret)
```

Now, `caret` is rather fussy about the type of categorical variables. The most straightforward procedure is to convert categorical all to `factor` type in pre-processing data.

```{r}
mortgage_data <- mortgage_data %>%
  mutate(employed = as_factor(if_else(employed, "t", "f")),
         accepted = as_factor(if_else(accepted, "t", "f")),
         employed = relevel(employed, ref = "f"),
         accepted = relevel(accepted, ref = "f")) 
```

First, we set up a `caret` `trainControl()` object. This controls the subsequent training of the classifier. Here we elect to do `repeatedcv` with `number = 5` and `repeats = 100`, i.e. break the data into five folds, use each fold for testing once and as part of the training set four times, gathering performance statistics each time, and repeat this splitting into folds 100 times. 

The `summaryFunction = twoClassSummary` argument tells the `train()` function to retain performance statistics for a binary classifier: in this case, $AUC$, $sensitivity$ and $specificity$ values, the latter two for a default threshold of $0.5$.

```{r}
train_control <- trainControl(method = "repeatedcv", 
                              number = 5,
                              repeats = 100,
                              savePredictions = TRUE, 
                              classProbs = TRUE, 
                              summaryFunction = twoClassSummary)
```

Now we `train()` the model (note, this may take half a minute or so to run!). We tell `train()` to fit a `glm` model, passing in the parameters to select logistic regression, i.e. `family = binomial(link = 'logit')`

```{r}
model <- train(accepted ~ tu_score + employed + age,
               data = mortgage_data,
               trControl = train_control,
               method = "glm",
               family = binomial(link = 'logit'))
```

```{r}
summary(model)
model$result
```

Again, bear in mind here that the `results` stored in the model object assume a threshold of $0.5$, and the $AUC$ values is, rather confusingly, labelled as `ROC`. Standard deviations of the performance values are also presented. 

We can see these values are pretty close to what we saw above: 88% performance is pretty good! 


# Expected value, cost-benefit analysis and optimal threshold

Here is a hypothetical analysis to help you understand how the **benefits** of true positive and true negative classifications and the **costs** of false positive and false negative classifications can be combined to set an **optimal threshold** for your classifier. The costs and benefits can be expressed in any resource of interest to an organisation: money, time and/or more abstract measures of reputation or prestige. 

To make progress, we have to understand the probability concept of **expected value**:

<br>
<div class='emphasis'>
The **expected value** of a random variable is the sum of the product of each value the variable can take with the probability of that value. For example, if we have a variable that can take three values $1$, $2$ and $3$ with probabilities $p_1$, $p_2$ and $p_3$, respectively, the expected value of the variable will be

$$\textrm{exp. value} = \sum_i \textrm{prob}_i \times v_i = 1 \times p_1 + 2 \times p_2 + 3 \times p_3$$

If all the values were equally likely (i.e. $p_1 = p_2 = p_3 = \frac{1}{3}$) then the expected value would be 

$$1 \times \frac{1}{3} + 2 \times \frac{1}{3} + 3 \times \frac{1}{3} = 2$$
</div>
<br>

We're going to use expected value to help us set an optimal threshold for a classifier. Let's stick with the mortgage application example from above. Now, say you work for a company that wish to target a referral campaign at potential mortgage applicants likely to have their applications accepted. Your company earns a modest referral fee from a client bank for each referred applicant who goes on to have their application accepted, but there are costs involved in making an approach to each potential applicant. 

Business analysis reveals the following costs and benefits:

* a **true positive** (a highly eligible applicant correctly identified as such) profits the company a referral fee of £25 from the client bank, and £5 less in contact and administration costs. We'll call this the **true positive profit** $TPP$
* a **true negative** (a low eligibility applicant correctly identified as such) earns no profit, as no approach is made. We'll call this the **true negative profit** $TNP$  
* a **false positive** (a low eligibility applicant misidentified as highly eligible) 'profits' the company -£5 due to contact and administration costs (note here, this is a **negative profit**). Let's call this the **false positive profit** $FPP$
* a **false negative** (a highly eligible applicant misidentified as low eligibility) earns no profit, as no approach is made. We'll call this the **false negative profit** $FNP$

Now, it turns out that the **expected profit per potential applicant** can be written as:

$$
\begin{aligned}
\textrm{exp. profit} = \textrm{prob}(\textrm{pos}) \times [TPR \times TPP + FNR \times FNP] \\ + \textrm{prob}(\textrm{neg}) \times [TNR \times TNP + FPR \times FPP]
\end{aligned}
$$

We can use the `classifier_data` object we calculated earlier from the `roc()` function to get the four rates we need

```{r}
classifier_data <- classifier_data %>%
  rename(
    tpr = sensitivity,
    tnr = specificity,
  ) %>%
  mutate(
    fpr = 1 - tnr,
    fnr = 1 - tpr
  )

classifier_data
```

But we need to get $\textrm{prob}(\textrm{pos})$ and $\textrm{prob}(\textrm{neg})$ from the original data.

```{r}
prob_pos = sum(mortgage_data$accepted == "t") / 1000
prob_pos
prob_neg = sum(mortgage_data$accepted == "f") / 1000
prob_neg
```


```{r}
# profits
tpp <- 20
tnp <- 0
fpp <- -5
fnp <- 0

classifier_data <- classifier_data %>%
  mutate(
    exp_profit_per_pot_applicant = 
      prob_pos * (tpr * tpp + fnr * fnp) + 
      prob_neg * (tnr * tnp + fpr * fpp)
  )

classifier_data %>%
  ggplot(aes(x = threshold, y = exp_profit_per_pot_applicant)) +
  geom_line()

classifier_data %>%
  filter(exp_profit_per_pot_applicant == max(exp_profit_per_pot_applicant))
```

So maximum expected profit per potential applicant is achieved with a classifier threshold of $0.196$. On average, we expect every potential applicant run through the classifier to earn the company £3.83.

# Recap


* What is the ROC curve of a binary classifier? What would a 'random guess' classifier look like on this plot? What about a 'perfect' classifier?
<details>
<summary>**Answer**</summary>
  * The ROC curve is a plot of $TPR$ against $FPR$ as a function of threshold probability. 
  * A 'random guess' classifier would appear as a straight line along the rising diagonal. 
  * A 'perfect' classifier would appear as two straight lines pointing to the top left corner of the plot (i.e. $TPR = 1$ and $FPR = 0$
</details>

<br>

* What is the $AUC$ of a ROC curve? What would the $AUC$ value be for a 'random guess' classifier? What about a 'perfect' classifier?
<details>
<summary>**Answer**</summary>
  * The $AUC$ is the area under the curve of a ROC curve, literally, the area enclosed between the curve and the $FPR$ axis.  
  * A 'random guess' classifier would have an $AUC$ of $0.5$.
  * A 'perfect' classifier would have an $AUC$ of $1$.
</details>

<br>

* What is the Gini coefficient?
<details>
<summary>**Answer**</summary>
This is just a normalisation of the AUC so that a random classifier has value $0$, and a perfect classifier has value $1$. The definition in terms of $AUC$ is $\textrm{Gini} = 2 \times AUC - 1$ 
</details>

<br>

* What is expected value?
<details>
<summary>**Answer**</summary>
The expected value of a random variable is the sum of the product of each possible value $v_i$ of the variable with the probability $\textrm{prob}_i$ of that value.
$$\textrm{exp. value} = \sum_i \textrm{prob}_i \times v_i$$
</details>


<br>



