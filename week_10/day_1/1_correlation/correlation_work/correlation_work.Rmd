---
title: "Correlation"
author: "Jonny Nelson"
date: "17/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(janitor)
library(tidyverse)
```

```{r}
noisy_bivariate <- function(noise = 1, gradient = 1){
  x <- runif(n = 200, min = 0, max = 10)
  y <- gradient * x + 10
  y_scatter <- noise * 4 * rnorm(n = 200)
  y <- y + y_scatter
  data = tibble(x, y)

  r <- round(cor(x, y), 4)
  title <- paste(
    "noise = ", noise,
    ", gradient = ", gradient,
    ", r = ", r
  )
  
  data %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    xlim(0, 10) +
    ylim(min(c(min(y), 0)), max(c(max(y), 10))) +
    ggtitle(title)
  
}

noisy_bivariate(noise = 2, gradient = 3)

# Change noise and gradient values to change the "r" value
```

## correlation calculation in R

```{r}
mtcars %>%
  summarise(cor = cor(mpg, wt))

# Correlation between mpg and weight in mtcars data set

cor(mtcars$mpg, mtcars$wt)

# Does not matter on the way around they are in the correlation calculation 
```

## Anscombe shows us data needs to be visualised as correlation is a blunt tool

```{r}
anscombe

cor(anscombe$x1, anscombe$y1)

cor(anscombe$x2, anscombe$y2)

cor(anscombe$x3, anscombe$y3)

cor(anscombe$x4, anscombe$y4)
```

## Task

```{r}
state.x77

tibble_states <- clean_names(as_tibble(state.x77))

tibble_states

tibble_states %>%
    ggplot(aes(x = income, y = murder)) +
    geom_point()

tibble_states %>%
  summarise(cor = cor(income, murder))
```

```{r}
library(GGally)
library(psych)

psych::pairs.panels(state.x77)

ggscatmat(state.x77)

ggpairs(tibble_states)

# Different ways for visualising the variables and their correlation coefficients
```

# Statistics

```{r}
# y vs x
# independant variable vs dependant variable

y = ax + b
y = mx + b 
```

```{r}
line <- function(x,a,b) {
  a * x + b
}

data <- tibble(x = seq(-5, 5, 0.1),
               y = line(x, a = 0, b = 7))

data %>%
  ggplot(aes(x, y)) +
  geom_line(col = "red") +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0)


```

```{r}
noisy_line <- read_csv("data/noisy_line.csv")

centroid <- noisy_line %>%
  summarise(x = mean(x),
            y = mean(y))

centroid

noisy_line_plot <- noisy_line %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_point(x = centroid$x, y = centroid$y, colour = "red", size = 5)

noisy_line_plot
```

```{r}
centroid <- noisy_line %>%
  summarise(x = mean(x),
            y = mean(y))

get_intercept <- function(slope, centroid_x, centroid_y) {
  centroid_y - slope * centroid_x
}

slope <- centroid$y / centroid$x

slope

noisy_line_plot +
  geom_abline(slope = slope, 
              intercept = get_intercept(slope = slope,
                                        centroid$x,
                                        centroid$y))

```

```{r}
noisy_line_plot +
  geom_smooth(method = "lm")
```

## Lets do a regression

```{r}
sample <- tibble(
  height = c(176, 164, 181, 168, 195, 185, 166, 180, 188, 174),
  weight = c(82, 65, 85, 76, 90, 92, 68, 83, 94, 74 )
)

sample %>%
  ggplot(aes(weight, height)) +
  geom_point()

line

line <- function(x, b0, b1) {
  b0 + x * b1
}

sample <- sample %>%
  mutate(fit_height = line(weight, b0 = 95, b1 = 1))

sample %>%
  ggplot(aes(x = weight, y = height)) +
  geom_point(aes(weight, height)) +
  geom_point(aes(weight, fit_height), shape = 1) +
  geom_abline(slope = 1, intercept = 95, colour = "red") +
  geom_segment(aes(xend = weight, yend = fit_height))

# global options -> graphics -> backend = Cairo PNG 

# _*This Gives a Smooth Line*_
```

```{r}
sample <- sample %>%
  mutate(residuals = height - fit_height)

sample %>%
  summarise(sum_residuals = sum((residuals)))

sample <- sample %>%
  mutate(sq_residuals = residuals^2)

sample %>%
  summarise(sum_of_squares_res = sum(sq_residuals),
            standard_error = sqrt(sum_of_squares_res))
```

## create first linear model

```{r}
class(~weight)

# Modelling height as a function of weight

new_model <- lm(height ~ weight, sample)

# Predicting using our linear model

predict(new_model)

predict_at <- data.frame(weight = 78)

predict(new_model, newdata = predict_at)
```

```{r}
library(modelr)

sample <- sample %>%
  # select(-fit_height, -residuals, -sq_residuals) %>%
  add_predictions(new_model) %>%
  add_residuals(new_model)

sample %>%
  ggplot(aes(x = weight)) +
  geom_point(aes(y = height)) +
  geom_line(aes(y = pred), colour = "red")
  
```

```{r}
# Can see predictions for weights from 50:120

weights_predict <- tibble(
  weight = 50:120
)

weights_predict %>%
  add_predictions(new_model)

# Predicting outside the range of data as it may not remain as a linear regression
```

## Regression Diagnostics

```{r}
sample <- select(sample, height, weight)

model <- lm(height ~ weight, sample)

summary(model)

model %>%
  ggplot() +
  geom_boxplot(aes(x = weight, y = height))
```

```{r}
library(broom)

summary(model)

# Take the summary and put into a data frame

tidy_output <- clean_names(tidy(model))

glance_output <- clean_names(glance(model))
```

__r2__ the proportion of the variation that can be _explained_ by variation in the predictor var'.

H0: No effect
HA: Positive effect for weight


## Calculating m manually 

```{r}
x_dev <- sample$weight - mean(sample$weight)
y_dev <- sample$height - mean(sample$height)

top <- sum(x_dev * y_dev) / 9
bottom <- sum(x_dev^2 / 9)

model

top / bottom

cov(sample$weight, sample$height) /
var(sample$weight)
```




