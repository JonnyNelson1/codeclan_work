---
title: "spark_setup_work"
author: "Jonny Nelson"
date: "11/03/2022"
output: html_document
---

# Installing sparklyr and Spark

```{r}
library(sparklyr)
spark_install(version = "2.4.5")
```

# Testing the Spark Installation 

```{r}
sc <- spark_connect(master = "local")
spark_cars <- copy_to(sc, mtcars)
```
# What other versions of Spark are avaliable?

```{r}
library("sparklyr")
spark_available_versions(show_minor = TRUE)

# Quite a number avaliable
```

# What version of Spark are we running

```{r}
spark_installed_versions()
# Spark 2.4.5 
```

# Connect to Spark

```{r}
sc <- spark_connect(master = "local", version = "2.4.5")
# Using our own computer as the master node of the Spark cluster and in this case it is the only node. Not very impressive :(
```

# Check out the connection object "sc"

```{r}
sc
```

* Not a very impressive show with just the one cluster running Spark. * We can however use AWK or Microsoft Azure to add slave nodes into our network for faster processing.
* Can also create Droplets (nodes) on DigitalOcean, install Spark on these nodes and set up intracluster communications.
* Alternatively use Docker to set up a cluster - easier option.
* Databricks is free and sets up and runs instances for you on Azure and AWS. Great way to learn cloud computing! Own notebook format for using Spark also!!!

# Using data with Spark

```{r}
library(tidyverse)
library(janitor)

avocado<- read_csv("data/avocado.csv")
```

# Copying the data over to Spark

```{r}
avocado <- avocado %>%
  clean_names()

avocado_spark <- copy_to(sc, avocado, overwrite = TRUE)

# Alternative way to load the csv into Spark
# avocado_spark <- spark_read_csv(sc, path = "data/avocado.csv")
```

# Can also view tables in our Spark session 

```{r}
src_tbls(sc)
```
# Class of Spark Object

```{r}
class(avocado_spark)
# We don't acquire the data until the processing need arises
```

# Let's see what the data contains 

```{r}
str(avocado_spark)
# No data here
```

```{r}
library(pryr)
object_size(avocado)
object.size(avocado_spark)
```
# Just getting one column

```{r}
selected_avocado_spark <- avocado_spark %>% 
  select(average_price)

object_size(selected_avocado_spark)
```

# Importing the data from Spark

```{r}
collected_avocado_spark <- avocado_spark %>%
  collect()
  
object_size(collected_avocado_spark)
```

# Compute function to force Spark to run all operations and then store the resulting data in a Spark DataFrame.

```{r}
avocado_without_index <- avocado_spark %>%
  select(-x1) %>%
  compute("avocado_without_index")

src_tbls(sc)

# Can be useful to store summarised results without the whole data set coming through to R.
```

# Spark SQL and show_query()

* Most dplyr operations are being converted to Spark SQL commands to be run on the Spark DataFrame - Let's see.

```{r}
avocado_without_index %>%
  select(average_price) %>%
  show_query()

# Nice little SQL query there
```

# SQL Query

```{r}
library(DBI)
dbGetQuery(conn = sc, statement = "SELECT COUNT(*) FROM avocado")
```

# Another example of the SQL query

```{r}
avocado_without_index <-  avocado_without_index %>%
  mutate(high_average = average_price > 1.40) %>%
  show_query()
```

# Proof that the above query worked

```{r}
avocado_without_index %>%
  select(high_average) %>%
  glimpse()
```

# Task 1

```{r}
result <- avocado_without_index %>%
  group_by(region) %>%
  summarise(mean_price = mean(average_price)) %>%
  show_query()

class(result)
# still a tbl_spark object

summary_table <- result %>%
collect()

class(summary_table)
```

# sparklyr helper functions and RStudio

```{r}
# Open a web tab to look at Spark requests
spark_web(sc)
```

```{r}
# Viewing the Spark logs
spark_log(sc, filter = "sparklyr")
```

# Examples of simple analysis operations

# Plotting

```{r}
avocado_spark %>%
  ggplot(aes(x = type)) +
  geom_bar()

# Had to bring the whole data set into R to do this!!!!!
```

# Better way to visualise with R is do make Spark do the heavy lifting

```{r}
avocado_spark %>%
  count(type) %>%
  collect() %>%
  ggplot(aes(x = type, y = n)) +
  geom_col()

# Way faster

# We are essentially doing the count in Spark, importing that summarised data and then visualising in R
```

# Simple Modelling

```{r}
fit <- avocado_spark %>% 
  ml_linear_regression(average_price ~ total_volume)

class(fit)

# by using the ml_linear_regression() function we are putting the computation onto Spark, instead of using lm()
```

# Looking at the model

```{r}
library(modelr)
tidy(fit)

glance(fit)
```

# Taking a look at the fitted values

```{r}
library(broom)
augment(fit)
```

# More complex interplay of sparklyr and tidyverse

```{r}
fit %>%
  ml_predict(copy_to(sc, data.frame(total_volume = seq(3e7, 6e7, 1e6)))) %>%
  transmute(total_volume = total_volume, average_price = prediction) %>%
  mutate(source = "predict") %>%
  sdf_bind_rows(
    select(avocado_spark, total_volume, average_price) %>%
      mutate(source = "train")
  ) %>%
  collect() %>%
  ggplot(aes(x = total_volume, y = average_price, col = source)) +
  geom_point(alpha = 0.5)
```

# Correlations

```{r}
# Collecting the data we need
library(corrr)

corr_mat <- avocado_spark %>%
  select(-c("x1", "date", "type", "region")) %>%
  ml_corr() %>%
  collect() 
```

```{r}
# Cleaning up the correlation table
corr_mat <- corr_mat %>%
  mutate(rowname = names(corr_mat)) %>%
  column_to_rownames(var = "rowname")

corr_mat
```

```{r}
corr_mat %>%
  rplot(print_cor = TRUE)
```

# Spark Streaming

```{r}
# creates a folder called "input"
dir.create("input")

# Sampling 100 rows and writing the data as a .csv into the input folder.
avocado %>%
  sample_n(100) %>%
  write_csv("input/avocados_1.csv")
```

# Spark Stream to check for changes

```{r}
stream <- stream_read_csv(sc, "input/") %>%
    select(total_volume, average_price) %>%
    stream_write_csv("output/")
```

# See if we have anything in the output dir.

```{r}
Sys.sleep(20) # add this just to give time for stream to run while knitting to html
dir("output", pattern = ".csv")
```

* We do. Spark has processed the incoming csv and written an output csv. Let's write some more data to input.

# More data to input

```{r}
avocado %>%
  sample_n(100) %>%
  write_csv("input/avocados_2.csv")
```

# Checking

```{r}
Sys.sleep(20) # add this just to give time for stream to run while knitting to html
dir("output", pattern = ".csv")
```

* Now we have two .csv files. 

# Stop Stream

```{r}
stream_stop(stream)
```



# Disconnecting from Spark

```{r}
# disconnect our individual connection
spark_disconnect(sc)

# or disconnect all open connections
spark_disconnect_all()
```

