---
title: "spark_setup_work"
author: "Jonny Nelson"
date: "11/03/2022"
output: html_document
---

# Installing sparklyr and Spark

```{r}
library(sparklyr)
spark_install(version = "2.4.5")
```

# Testing the Spark Installation 

```{r}
sc <- spark_connect(master = "local")
spark_cars <- copy_to(sc, mtcars)
```
# What other versions of Spark are avaliable?

```{r}
library("sparklyr")
spark_available_versions(show_minor = TRUE)

# Quite a number avaliable
```

# What version of Spark are we running

```{r}
spark_installed_versions()
# Spark 2.4.5 
```

# Connect to Spark

```{r}
sc <- spark_connect(master = "local", version = "2.4.5")
# Using our own computer as the master node of the Spark cluster and in this case it is the only node. Not very impressive :(
```

# Check out the connection object "sc"

```{r}
sc
```

* Not a very impressive show with just the one cluster running Spark. * We can however use AWK or Microsoft Azure to add slave nodes into our network for faster processing.
* Can also create Droplets (nodes) on DigitalOcean, install Spark on these nodes and set up intracluster communications.
* Alternatively use Docker to set up a cluster - easier option.
* Databricks is free and sets up and runs instances for you on Azure and AWS. Great way to learn cloud computing! Own notebook format for using Spark also!!!

# Using data with Spark

```{r}
library(tidyverse)
library(janitor)

avocado<- read_csv("data/avocado.csv")
```

# Copying the data over to Spark

```{r}
avocado <- avocado %>%
  clean_names()

avocado_spark <- copy_to(sc, avocado, overwrite = TRUE)

# Alternative way to load the csv into Spark
# avocado_spark <- spark_read_csv(sc, path = "data/avocado.csv")
```

# Can also view tables in our Spark session 

```{r}
src_tbls(sc)
```
# Class of Spark Object

```{r}
class(avocado_spark)
# We don't acquire the data until the processing need arises
```

# Let's see what the data contains 

```{r}
str(avocado_spark)
# No data here
```

```{r}
library(pryr)
object_size(avocado)
object.size(avocado_spark)
```
# Just getting one column

```{r}
selected_avocado_spark <- avocado_spark %>% 
  select(average_price)

object_size(selected_avocado_spark)
```

# Importing the data from Spark

```{r}
collected_avocado_spark <- avocado_spark %>%
  collect()
  
object_size(collected_avocado_spark)
```

# Compute function to force Spark to run all operations and then store the resulting data in a Spark DataFrame.

```{r}
avocado_without_index <- avocado_spark %>%
  select(-x1) %>%
  compute("avocado_without_index")

src_tbls(sc)

# Can be useful to store summarised results without the whole data set coming through to R.
```

# Spark SQL and show_query()

* Most dplyr operations are being converted to Spark SQL commands to be run on the Spark DataFrame - Let's see.

```{r}
avocado_without_index %>%
  select(average_price) %>%
  show_query()

# Nice little SQL query there
```



```{r}
library(DBI)
dbGetQuery(conn = sc, statement = "SELECT COUNT(*) FROM avocado")
```

